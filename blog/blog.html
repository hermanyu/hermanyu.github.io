<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="blog.css">
    <base target="_blank">
</head>
<body>
    <div class="flex-container">
        <div class="header-tabs">
            <div class="tab statistics selected-tab">
                <p>Statistics</p>
            </div>
            <div class="tab machine-learning">
                <p>Machine Learning</p>
            </div>
            <div class="tab misc">
                <p>Misc</p>
            </div>
        </div>
        <div class="page-lists">
            <div class="page-list statistics selected-list">
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="blog-pages/statistics/statistical-models-and-dgps.html">
                        <div class="entry-title">
                            <p>Statistical Models and DGPs</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                The most fundamental application of statistics is to use finite sample data to estimate some data generating process (DGP).
                                If we want to understanding DGPs, we need to actually generate data ourselves. The topics in this section deal with various 
                                probability models (how random variables are defined) and how those model generate finite sample data (statistical models). 
                                Understanding such models has a very concrete application: engineering synthetic data sets. 
                            </p>
                        </div>
                    </a>
                </div>
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="https://hermanyu.github.io/design-of-experiments/">
                        <div class="entry-title">
                            <p>Design And Analysis of Experiments (DOE)</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                Typically, there is some real-world DGP, whose exact nature is unknown. 
                                To understand this DGP, we will need to estimate the underlying causal relationships using finite sample data.
                                The "gold standard" for estimating causality is a randomized controlled trial (RCT); RCTs are machines that can be tuned to 
                                navigate various tradeoffs in a model: sample size vs power, number of causal relationships vs false positive rates, confounding variables vs model complexity, etc.
                                Design of experiments is the study of how to engineer and analyze RCTs to navigate these tradeoffs.
                            </p>
                        </div>
                    </a>
                </div>
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="https://hermanyu.github.io/forecasting/">
                        <div class="entry-title">
                            <p>Forecasting</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                <em>Time-series data</em> is data which is indexed by time. Samples from time-series data is almost never independent: the stock price at time 12:59 pm
                                is correlated with the stock price at 1:00 pm. This property where data points are correlated based on recency is called <em>serial correlation</em>.
                                Since serial correlation violates the independence assumption needed for traditional statistical methods, 
                                a new theory is needed to estimate the DGP from a finite sample of time-series data. 
                                Correlations in time might cause headaches for estimating the DGP, BUT there is a major benefit: 
                                correlations between past vs future make it possible to <em>forecast</em> future values.
                        </div>
                    </a>
                </div>
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="blog-pages/statistics/bayesian-models.html">
                        <div class="entry-title">
                            <p>Bayesian Models</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                Often times, we will have very specific domain knowledge which places certain restrictions on what 
                                the DGP can and cannot be. Bayesian models allow us to specify a prior distributions to encapsulate
                                our understanding of what we consider "reasonable" beliefs about our DGPs.
                            </p>
                        </div>
                    </a>
                </div>
            </div>
<!-- PAGE BREAK.................................................................................................................................... -->
            <div class="page-list machine-learning">
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="blog-pages/statistics/data-generation-processes.html">
                        <div class="entry-title">
                            <p>Linear and Additive Regression</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                Linear are parametric regression models from statistics which can 
                                be used to generate predictions for a quantity of interest. The main advantage of linear 
                                models is their interpretability and easiness to compute, but they present 2 main drawbacks. 
                                Drawback 1: they do not a priori capture non-linear behavior. Drawback 2: they do not a priori capture 
                                interactions between predictor variables. Because of these drawbacks, the user will typically have to 
                                manually engineer new features by creating non-linear terms and interaction terms. Drawback 1 can be 
                                decently resolved by generalizing to Generalized Additive Models (GAMs), but there is no getting around Drawback 2.
                            </p>
                        </div>
                    </a>
                </div>
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="blog-pages/statistics/parametric-models.html">
                        <div class="entry-title">
                            <p>Logistic Regression</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                Logistic regression is really a linear regression applied to a transformed binary outcome variable. 
                                Logistic regression is mostly used for classification tasks, which presents interesting challenges 
                                when it comes to evaluating model performance.
                            </p>
                        </div>
                    </a>
                </div>
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="blog-pages/statistics/nonparametric-models.html">
                        <div class="entry-title">
                            <p>Naive Bayes</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                Naive Bayes is a classification technique which uses conditional group means to 
                                determine group membership of each individual data points. If a data point X has features that 
                                are very abnormal for group A, it stands to reason that X is not likely to be a part of group A. 
                                Naive Bayes thus attempts to quantify how "abnormal" it is a data point to be a member of each group. 
                                This "abnormal"-ness is computed using Bayes theorem and by making a "naive" assumption that 
                                the input features are all independent from each other (hence the name: Naive Bayes).
                            </p>
                        </div>
                    </a>
                </div>
                <div class="list-entry text-bubble">
                    <a class="blog-link" href="blog-pages/statistics/nonparametric-models.html">
                        <div class="entry-title">
                            <p>Discriminant Analysis</p>
                        </div>
                        <div class="entry-body">
                            <p>
                                
                            </p>
                        </div>
                    </a>
                </div>
            </div>
        </div>
    </div>
<!-- PAGE BREAK.................................................................................... -->
    <div class="page-list misc">
    </div>
    <!-- 
        putting the script element in the head causes
        the .js script to run BEFORE the DOM gets loaded.
        This will cause querySelectorAll() to fail to find 
        any matching elements and return an empty NodeList.
    -->
    <script src="javascript/selectTab.js"></script>
</body>
</html>